{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dab518",
   "metadata": {},
   "source": [
    "# Objective Video Quality Indicators (VQIs): Hands‚ÄëOn Lab (90 minutes)\n",
    "\n",
    "**Topic:** Computing objective, no‚Äëreference indicators for images/videos  \n",
    "**Tools:** `agh_vqis` (AGH Video Quality Indicators) 3.4.6+, `ffmpeg` 4.4.2+, Python 3.12\n",
    "\n",
    "This notebook guides you through setting up the environment, generating test content, computing VQIs, exporting results to CSV, and analyzing indicator time‚Äëseries. You will also compare VQIs to simple full‚Äëreference metrics (PSNR/SSIM) on synthetic data to understand differences between no‚Äëreference and full‚Äëreference approaches.\n",
    "\n",
    "> **Credits:** Indicators and Python wrapper originate from the AGH Video Quality of Experience (QoE) Team. If you use individual indicators in reports, cite the corresponding papers listed on [the project page](https://qoe.agh.edu.pl/indicators/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38324c",
   "metadata": {},
   "source": [
    "## Learning outcomes\n",
    "By the end of this lab you will be able to:\n",
    "- Install and verify a no‚Äëreference IQI/VQI toolkit (`agh_vqis`).\n",
    "- Run VQIs on single files and on folders (batch mode).\n",
    "- Enable/disable specific indicators and use *casting* to standardize resolution for selected VQIs.\n",
    "- Export and interpret per‚Äëframe CSV results; visualize time‚Äëseries and detect events (e.g., freezing, flicker, letter/pillar‚Äëboxing).\n",
    "- Compare no‚Äëreference indicators with reference metrics (PSNR/SSIM) on synthetic data and discuss practical differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f65817",
   "metadata": {},
   "source": [
    "## Environment (UNIX recommended)\n",
    "Run the following in **your UNIX** terminal:\n",
    "\n",
    "```bash\n",
    "# üêß Ubuntu / Linux\n",
    "sudo apt update\n",
    "sudo apt install -y ffmpeg python3.12 python3.12-venv python3-pip\n",
    "python3 -m venv ~/.venvs/vqis\n",
    "\n",
    "# üçé macOS\n",
    "brew install python@3.12 ffmpeg\n",
    "/opt/homebrew/bin/python3.12 -m venv ~/.venvs/vqis\n",
    "\n",
    "source ~/.venvs/vqis/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install agh_vqis numpy pandas matplotlib scikit-image tqdm\n",
    "```\n",
    "\n",
    "Verify versions:\n",
    "```bash\n",
    "ffmpeg -version | head -n1\n",
    "python --version\n",
    "python -c \"import agh_vqis, sys;print('agh_vqis:',getattr(agh_vqis,'__version__','unknown'))\"\n",
    "```\n",
    "\n",
    "> **Note:** `agh_vqis` requires `ffmpeg >= 4.4.2` and Python 3.12. The package supports common video/image formats (mp4, mkv, webm, avi, jpg, png, bmp, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e6205",
   "metadata": {},
   "source": [
    "## AGH VQIs: set exec bit\n",
    "\n",
    "```bash\n",
    "# 1) Locate the directory containing agh_vqis binaries\n",
    "BIN_DIR=$(python3 - <<'PY'\n",
    "import pathlib, agh_vqis\n",
    "print(pathlib.Path(agh_vqis.__file__).parent / 'binaries')\n",
    "PY\n",
    ")\n",
    "\n",
    "# 2) Check permissions and make binaries executable\n",
    "ls -alh \"$BIN_DIR\"\n",
    "chmod +x \"$BIN_DIR\"/*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775832c",
   "metadata": {},
   "source": [
    "## Quick sanity check: generate a synthetic test clip with `ffmpeg`\n",
    "We will create three short clips with built‚Äëin FFmpeg generators:\n",
    "- **clean.mp4** ‚Äì moving color bars (baseline);\n",
    "- **blur.mp4** ‚Äì bars with Gaussian blur;\n",
    "- **blocky.mp4** ‚Äì bars with heavy compression (visible blocking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16911867",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail   # e: exit on error, u: treat unset vars as error, o pipefail: fail on first error in a pipeline\n",
    "mkdir -p data\n",
    "cd data\n",
    "\n",
    "# 5s @ 30 fps, 1280x720 moving bars\n",
    "ffmpeg -y -f lavfi -i testsrc2=size=1280x720:rate=30 -t 5 -c:v libx264 -pix_fmt yuv420p clean.mp4 -hide_banner -loglevel error\n",
    "\n",
    "# blur version: apply gaussian blur\n",
    "ffmpeg -y -i clean.mp4 -vf \"gblur=sigma=4\" -c:v libx264 -pix_fmt yuv420p blur.mp4 -hide_banner -loglevel error\n",
    "\n",
    "# blocky version: strong quantization\n",
    "ffmpeg -y -i clean.mp4 -c:v libx264 -crf 40 -preset veryfast -pix_fmt yuv420p blocky.mp4 -hide_banner -loglevel error\n",
    "\n",
    "ls -lh *.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69182c",
   "metadata": {},
   "source": [
    "## Visual preview of generated test videos\n",
    "\n",
    "Before computing any objective indicators, let‚Äôs visually inspect the synthetic clips we have just created.\n",
    "They represent three simple types of distortions that we will later analyze numerically:\n",
    "\n",
    "- **clean.mp4** ‚Äì reference clip with moving color bars (no distortion)  \n",
    "- **blur.mp4** ‚Äì blurred version (simulated defocus)  \n",
    "- **blocky.mp4** ‚Äì heavily compressed version with visible blocking artifacts\n",
    "\n",
    "> These previews are for quick visual intuition: what kinds of artifacts are present and how they differ perceptually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "\n",
    "display(Video('data/clean.mp4', embed=True, width=640))\n",
    "display(Video('data/blur.mp4',  embed=True, width=640))\n",
    "display(Video('data/blocky.mp4', embed=True, width=640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fbec3",
   "metadata": {},
   "source": [
    "You can also use some different video player, like VLC Media Player (install it if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9d0c0",
   "metadata": {},
   "source": [
    "## Compute VQIs with `agh_vqis` (single file)\n",
    "The functions below come from the package:\n",
    "\n",
    "```python\n",
    "from agh_vqis import process_single_mm_file, VQIs\n",
    "```\n",
    "\n",
    "- By default, all indicators are enabled **except** `blur_amount` (it can be slow).  \n",
    "- You can **disable** any indicator by passing `{VQIs.indicator_name: False}`.  \n",
    "- You can **enable** `blur_amount` by passing `{VQIs.blur_amount: True}`.\n",
    "\n",
    "The output is a CSV with per‚Äëframe values (one row per frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfe3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from agh_vqis import process_single_mm_file, VQIs\n",
    "import pandas as pd\n",
    "\n",
    "# Example: compute on clean.mp4, disable colourfulness and contrast; enable blur_amount\n",
    "# ‚ö†Ô∏è WARNING: 'blur_amount' is computationally heavy - this step may take a couple of minutes even for short clips.\n",
    "process_single_mm_file(\n",
    "    Path('data/clean.mp4'),\n",
    "    options={\n",
    "        VQIs.colourfulness: False,\n",
    "        VQIs.contrast: False,\n",
    "        VQIs.blur_amount: True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load and preview resulting CSV\n",
    "df = pd.read_csv(\"VQIs_for_clean.csv\")\n",
    "print(f\"Loaded results from: {csv_file} (shape={df.shape})\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b83c5",
   "metadata": {},
   "source": [
    "## Batch processing (a folder)\n",
    "Use `process_folder_w_mm_files(Path(...), options=...)` to produce one CSV per file in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab583ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from agh_vqis import process_folder_w_mm_files\n",
    "import pandas as pd\n",
    "\n",
    "# ‚ö†Ô∏è WARNING: Depending on the number and duration of videos,\n",
    "# this step may take several minutes ‚Äî each file is analyzed frame-by-frame.\n",
    "print(\"Running AGH VQIs on all .mp4 files in 'data/' folder...\")\n",
    "\n",
    "process_folder_w_mm_files(Path('data'))\n",
    "    \n",
    "csv_clean  = Path(\"VQIs_for_clean.csv\")\n",
    "csv_blur   = Path(\"VQIs_for_blur.csv\")\n",
    "csv_blocky = Path(\"VQIs_for_blocky.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ Finished. Generated CSV files:\")\n",
    "print(\" ‚Ä¢\", csv_clean)\n",
    "print(\" ‚Ä¢\", csv_blur)\n",
    "print(\" ‚Ä¢\", csv_blocky)\n",
    "\n",
    "print(\"\\nPreview of first 5 rows per file:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean  = pd.read_csv(csv_clean);   df_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blur   = pd.read_csv(csv_blur);    df_blur.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ab79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocky = pd.read_csv(csv_blocky);  df_blocky.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de279285",
   "metadata": {},
   "source": [
    "## Visualization: time‚Äëseries of selected indicators\n",
    "We'll parse one CSV and plot a few indicators (e.g., `blockiness`, `blur`, `flickering`, `freezing`, `letterbox`, `pillarbox`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adea616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Find a CSV produced above (pick the first)\n",
    "csvs = sorted(Path('.').rglob('*.csv'))\n",
    "assert csvs, 'No CSV results found. Run the processing cells first.'\n",
    "df = pd.read_csv(csvs[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few indicators over time (frame index)\n",
    "indicators_to_plot = [\n",
    "    c for c in df.columns if c.lower() in (\n",
    "        'blockiness','blur','flickering','freezing','letterbox','pillarbox','temporal activity','spatial activity'\n",
    "    )\n",
    "]\n",
    "if not indicators_to_plot:\n",
    "    indicators_to_plot = df.columns[1:6]  # fallback\n",
    "\n",
    "for col in indicators_to_plot:\n",
    "    plt.figure()\n",
    "    df[col].plot(title=col)\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8058ea",
   "metadata": {},
   "source": [
    "## Experimental: *Casting* selected VQIs to a target resolution\n",
    "Some indicators can be computed as if the input had a different resolution (\"casted\"). This can improve comparability across datasets.\n",
    "\n",
    "Example below casts **Blur** to 1440p and **Blockiness** to 2160p for `blur.mp4`. See package docs for available casts and expected correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61458aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from agh_vqis import process_single_mm_file, CastVQI, DestResolution\n",
    "import pandas as pd\n",
    "\n",
    "# Casts valid for 720p sources (no Blockiness cast from 720p)\n",
    "process_single_mm_file(\n",
    "    Path('data/blur.mp4'),\n",
    "    options={\n",
    "        CastVQI.blur: DestResolution.p1080,       # ‚úÖ 720p ‚Üí 1080p (Blur)\n",
    "        CastVQI.contrast: DestResolution.p1080,   # ‚úÖ 720p ‚Üí 1080p (Contrast)\n",
    "        CastVQI.exposure: DestResolution.p1080,   # ‚úÖ 720p ‚Üí 1080p (Exposure)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ignore returned 0 (exit code). Load the per-frame CSV written by agh_vqis:\n",
    "pd.read_csv(\"VQIs_for_blur.csv\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258fbd6",
   "metadata": {},
   "source": [
    "## No‚Äëreference vs. full‚Äëreference: quick comparison (PSNR/SSIM)\n",
    "We will create a **reference** clip and a **distorted** clip and compute PSNR/SSIM (full‚Äëreference) per frame to contrast with selected VQIs (no‚Äëreference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74064372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd data\n",
    "# reference: clean.mp4 already exists; create distorted: add noise + reencode\n",
    "ffmpeg -y -i clean.mp4 -vf \"noise=alls=60:allf=t+u\" -c:v libx264 -crf 35 -pix_fmt yuv420p noisy.mp4 -hide_banner -loglevel error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496145de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd data\n",
    "ls -lh noisy.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c476a",
   "metadata": {},
   "source": [
    "Before computing PSNR and SSIM, let‚Äôs visually inspect the two clips that will be compared.\n",
    "\n",
    "They represent a typical full-reference setup: one undistorted reference video and one distorted version of it.\n",
    "\n",
    "The distortions here come from artificial noise and re-encoding, which we‚Äôll later quantify using PSNR and SSIM metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17951ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "\n",
    "display(Video('data/clean.mp4', embed=True, width=640))\n",
    "display(Video('data/noisy.mp4', embed=True, width=640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm   # ‚úÖ works in terminal and Jupyter\n",
    "\n",
    "ref_reader = imageio.get_reader('data/clean.mp4')\n",
    "noisy_reader = imageio.get_reader('data/noisy.mp4')\n",
    "\n",
    "num_frames = min(len(ref_reader), len(noisy_reader))\n",
    "\n",
    "psnr_list, ssim_list = [], []\n",
    "for fidx, (fr, fd) in enumerate(tqdm(zip(ref_reader, noisy_reader),\n",
    "                                     total=num_frames,\n",
    "                                     desc=\"Comparing frames\")):\n",
    "    # grayscale for SSIM; PSNR on RGB\n",
    "    fr_gray = np.dot(fr[...,:3], [0.299, 0.587, 0.114]).astype(np.float32)\n",
    "    fd_gray = np.dot(fd[...,:3], [0.299, 0.587, 0.114]).astype(np.float32)\n",
    "    psnr_list.append(psnr(fr, fd, data_range=255))\n",
    "    ssim_list.append(ssim(fr_gray, fd_gray, data_range=255))\n",
    "\n",
    "df_ref = pd.DataFrame({'frame': np.arange(len(psnr_list)),\n",
    "                       'PSNR': psnr_list,\n",
    "                       'SSIM': ssim_list})\n",
    "df_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40511bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "df_ref['PSNR'].plot(title='PSNR over time')\n",
    "plt.xlabel('Frame'); plt.ylabel('PSNR [dB]'); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df_ref['SSIM'].plot(title='SSIM over time')\n",
    "plt.xlabel('Frame'); plt.ylabel('SSIM'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea760d48",
   "metadata": {},
   "source": [
    "### Discussion prompts\n",
    "- When **no‚Äëreference** VQIs report high blur/blockiness, how do PSNR/SSIM behave?\n",
    "- In which scenarios are **no‚Äëreference** indicators more practical than full‚Äëreference metrics?\n",
    "- What are typical *ranges* and *monotonicities* of selected VQIs (see the indicator table in the project page)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11f8c7",
   "metadata": {},
   "source": [
    "## Comparing no-reference and full-reference indicators\n",
    "\n",
    "Finally, let‚Äôs compare how objective no-reference indicators (e.g., Blur, Noise) correlate with full-reference metrics such as PSNR and SSIM.\n",
    "\n",
    "Although these metrics are based on different assumptions, visual correlation can show whether both methods capture similar quality trends.\n",
    "\n",
    "Scatter plots below illustrate this relationship, and the correlation matrix quantifies it numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load FR metrics (computed earlier)\n",
    "df_fr = df_ref.copy()  # PSNR + SSIM per frame\n",
    "\n",
    "# Load selected NR metrics (from agh_vqis output for noisy video)\n",
    "df_nr = pd.read_csv(\"VQIs_for_noisy.csv\")  # generated by agh_vqis\n",
    "# Example columns that correlate with PSNR/SSIM perception:\n",
    "#   - \"Blur\"  (more blur = lower quality)\n",
    "#   - \"Noise\" (higher noise = lower quality)\n",
    "#   - \"Contrast\" (higher contrast = often higher perceived quality)\n",
    "\n",
    "# Align data length (just in case)\n",
    "min_len = min(len(df_fr), len(df_nr))\n",
    "df_fr = df_fr.iloc[:min_len]\n",
    "df_nr = df_nr.iloc[:min_len]\n",
    "\n",
    "# Combine both into one DataFrame\n",
    "df_corr = pd.concat([df_fr.reset_index(drop=True),\n",
    "                     df_nr[['Blur', 'Noise', 'Contrast']].reset_index(drop=True)],\n",
    "                    axis=1)\n",
    "\n",
    "# Compute simple correlation matrix\n",
    "corr = df_corr.corr()\n",
    "\n",
    "print(\"Correlation matrix (selected NR vs FR metrics):\")\n",
    "display(corr.loc[['PSNR', 'SSIM'], ['Blur', 'Noise', 'Contrast']])\n",
    "\n",
    "# Scatter plots for visual correlation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].scatter(df_corr['Blur'], df_corr['PSNR'], alpha=0.6)\n",
    "axes[0].set_xlabel(\"Blur (NR)\"); axes[0].set_ylabel(\"PSNR (FR)\")\n",
    "axes[0].set_title(\"Blur vs PSNR\")\n",
    "\n",
    "axes[1].scatter(df_corr['Noise'], df_corr['SSIM'], alpha=0.6, color='orange')\n",
    "axes[1].set_xlabel(\"Noise (NR)\"); axes[1].set_ylabel(\"SSIM (FR)\")\n",
    "axes[1].set_title(\"Noise vs SSIM\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9fef7",
   "metadata": {},
   "source": [
    "## CLI usage (alternative) ‚Äì optional\n",
    "You can also run the package from the command line:\n",
    "\n",
    "```bash\n",
    "python3 -m agh_vqis data/clean.mp4\n",
    "python3 -m agh_vqis data/   # process the whole folder\n",
    "python3 -m agh_vqis -h      # help\n",
    "```\n",
    "\n",
    "This produces the same CSV outputs as the Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8727c22",
   "metadata": {},
   "source": [
    "## Mini‚Äëtasks (to be completed in class)\n",
    "1. **Run VQIs** on `clean.mp4`, `blur.mp4`, and `blocky.mp4`. Compare `blockiness`, `blur`, `flickering`, and `freezing` time‚Äëseries. Which indicators react as expected? Explain.\n",
    "2. **Enable** the `blur_amount` indicator and measure runtime overhead. Does it yield additional insights compared to `blur`?\n",
    "3. **Casting:** Cast `blur` to 1440p and `blockiness` to 2160p for `blocky.mp4`. Discuss the pros/cons of standardized resolution.\n",
    "4. **Event detection:** Implement a simple rule to detect **freezing** events from the indicator time‚Äëseries (e.g., threshold + minimum duration). Mark events on a plot.\n",
    "5. **Reference vs no‚Äëreference:** Using the PSNR/SSIM section, correlate trends with one chosen VQI. Where do they disagree, and why?\n",
    "6. **Batch:** Put 5 additional files (your own or generated with FFmpeg: different CRF, defocus blur, brightness/contrast changes) into `data/` and compute VQIs for all. Summarize key differences in a short table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af69eb",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "- **FFmpeg not found / too old:** `sudo apt update && sudo apt -y install ffmpeg`.\n",
    "- **CSV empty:** check that the file actually has frames; try a different codec/pix_fmt.\n",
    "- **Performance:** disable costly indicators (leave `blur_amount` off) or process fewer frames (`-t` in FFmpeg to cut clips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79d0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
